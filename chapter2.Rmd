# Week2 - Regression and model validation

I prepared myself for this weeks topic by completing DataCamp exercise on "Regression and model validation".

## Data Wrangling
For the data wrangling part of this weeks exercises I mostly used the methods learned from the DataCamp exercise. I learned the basics of data wrangling in R and know how to find out more methods/tools and how to use them to wrangle the data.


My source code used in the wrangling is [here](https://github.com/ittobor/IODS-project/blob/master/data/create_learning2014.R).  

Basically, here are the main points what is done in the code:

* Original dataset is read from given web source.
* Structure of the data is explored with `str` and `dim`.
* `learning2014` is created as new empty data.frame to store the required variables.
* Variables `gender`, `age`, `attitude` and `points` are simply copied from the orignal dataset to `learning2014`.
* The question variables from the original dataset are combined and stored as scaled variables in to `learning2014`. dplyr library is used here.
* Observations with `points < 1` are filtered out from `learning2014`.
* learning2014` data.frame is saved as a text-file with same format as the original dataset.

The dataset (`learning2014.txt`) that the code created can be found from [here](https://github.com/ittobor/IODS-project/blob/master/data/learning2014.txt).

## Data Analysis
```{r echo=FALSE}
library(ggplot2)
library(GGally)
learning2014 <- read.table("/Users/ittobor/Projects/iods/IODS-project/data/learning2014.txt", sep="\t", header=TRUE)
```

With `dim` function we learn the dimension of the input dataset. That is the function returns the number of columns and rows.
```{r echo=TRUE}
dim(learning2014)
```

With `str` function we learn about the structure of the input dataset. Basically we get to know the number of observations (rows) and variables (columns) in the dataset. The output summary from the function lists the variables, the type of each variable vector and few example values from the vector.
```{r echo=TRUE}
str(learning2014)
```

I use `ggpairs` to show detailed graphical overview of the input dataset, just like it was done in the DataCamp course for this weeks exercises. `ggpairs`creates matrix of plots from the given dataset. With aesthetics mapping we are instructing the colors to be based `gender` variable and the transparency is controlled with value give to alpha parameter. Parameter lower controls the type of plots for lower section of the plot matrix. All plots are between pair of variables in the `learning2014` dataset and each are categorized based on `gender` variable. Above the diagonal with density plots there are the correlation coeffiecients of the pairs as a whole and then categorized based on `gender` variable.

From the plots we can see that nearly 2/3 of the observations are from females, thus 1/3 are from males. When we look at the single variables categorized by gender, we see that variable `attitude` deviates the most between sexes. Also females tend to be more younger than males based on variable `age`. There is also deviations between female and male in the combined question variables, where `surface` question group deviates the most and `strategy` to some extent.  

When we look at the correlation coefficients we see that `points` and `attitude` correlate in agreement the most (0.437) in the dataset variables, with equally high for both females and males. Then `deep` and `surf` combined question variables correlate in disagreement the most (-0.324), with amongst all males the variables disagree (-0.622) clearly more than amongst females (-0.087). Then there are several pairs that have similar correlation in agreement (`age`,`strategic`), (`deep`,`attitude`) and (`points`,`strategic`), and several pairs that have similar correlation in disagreement (`strategic`,`surface`), (`points`,`surface`), (`attitude`,`surface`) and (`age`,`surface`).

```{r echo=TRUE}
ggpairs(learning2014, 
        mapping = aes(col = gender, alpha = 0.3), 
        lower = list(combo = wrap("facethist", bins = 20)))
```

With `summary` command we can see statistics of each variable in the dataset `learning2014`. For categorical variable `gender` we only get number of values for females and males. Then for each of the numerical varibles we get minimum, maximum, mean, median, 1st and 3rd quantiles.
```{r echo=TRUE}
summary(learning2014)
```

From the summary below we can see the explanatory variables (`attitude`, `age`, `stra`) I used to build the model with `lm` function. I reran the code several times with different combination of variables to find the most fit variables for the model by comparing the summaries of each of the models. Residuals are the difference between actual values of `points` and the predicted values of `points`. For most regressions the residuals should be normally distributed, where for goodness of the model can be estimated by the value of mean being close to 0. And here we are close to zero (0.3303). There is a shorthand for estimating the signifigance of the variables at the end of line for each variable in the Coefficients section of the summary. The more the stars at the end of the line the more significant the variable is, dot is still very good. And here we have *** for `attitude` and . for both `stra` and `age`. The p-value tells the probability of variable being NOT relevant, so small values here are good. And as we can see we have very low value for `attitude` and and still significantly low for both `stra` and `age`.  

R-squared value tells how close the data are to the fitted regression line. It indicates the percentage that the model explains of the variability of the response data around its mean. 0% when none, 100% when all of the variability is explained. Here we have 21%. In general the higher the R-squared the better the model fits the data. Low R-squared value are not always bad. E.g. on occasions when trying to predict human behavior R-squared values are typically lower than 50%. Here we have a model fitted to variables descibing mostly (results of) human behavior. Also, if R-squared is low but the variables are statistically significant important conclusion can still be drawn. And here we have low R-squared with statistically significant variables.
```{r echo=FALSE}
mlr_model <- lm(points ~ attitude + stra + age, 
                data = learning2014)
summary(mlr_model)
```

Model assumptions describe the data generating process. How well the assumptions fit reality the better the model describes the phenomenom of interest. With linear regression linearity is assumed: target is modelled as a linear combination of model parameters. Usually is also assumed that the errors are normally distributed, errors are not correlated and have constant variance, and that size of given error does not depend on the explanatory variables. 

With QQ-plot of the residuals we can explore the assumption that of errors of the model are normally distributed. Here we in the 2nd plot we have a QQ-plot of the model and we can see that the majority of the explanatory variables fit well to the line and to the normality assumption.  

The constant variance assumption can be explored with simple scatter plot of residuals versus model predictions. Any pattern on the scatter plot implies a problem with the assumption. Here the scatter plot is the 1st plot, and I must say that it is a bit difficult to say that is there a pattern (problem) in the plot especially when looking at with less fitted values and more fitted values. Then again, in between of less and more fitted values it is well scattered.  

Leverage measures how much impact a single observation has on the model and residuals vs. leverage plot helps identifying observations with high impact. Here the last plot is the residuals vs. leverage plot. Again it is little hard for me to tell, but my interpretation is that there is no single outstanding observation that would drag the model.
```{r echo=FALSE}
plot(mlr_model, which = c(1))
plot(mlr_model, which = c(2))
plot(mlr_model, which = c(5))
```