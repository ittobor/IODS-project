# Week4 - Clustering and classification

Also for this week I prepared myself by completing DataCamp exercise on "Clustering and classification".

## Data Analysis

### Step 1 - New markdown file

Step one again was to create chapter4.Rmd to inject its content to the actual course diary as a child of subscipts in index.Rmd.

```{r echo=FALSE, message=FALSE}
library(ggplot2)
library(GGally)
library(dplyr)
library(tidyr)
library(MASS)
library(corrplot)
library(tidyverse)
```

### Step 2 - THe Boston dataset from MASS package

```{r echo=FALSE, message=FALSE}
data("Boston")
```
In this exercise we are using the Boston dataset from MASS packege for R. The dataset contains "Housing Values in Suburbs of Boston" according to the dataset description in [here](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html) where you can also find details about the dataset columns.

Boston dataset is a class type of: `r class(Boston)`. The dimensions of the dataset are `r dim(Boston)[1]` observations (rows) with `r dim(Boston)[2]` variables (columns).  

Structure of the dataset is listed below.
```{r echo=FALSE, message=FALSE}
str(Boston)
```

### Step 3 - Graphical overview and the varible summaries of the Boston dataset 

First, with a summary, and together with the additional information available in  [here](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html), we can take a look at the different scales of each of the variables. There are different rates, ratios, proprotions, avarages, means, medians and dummy (boolen indicator) as values used for different variables.

```{r echo=FALSE, message=FALSE}
summary(Boston)
```

Second, we can look at the density plots of each variable. I was tempted to draw relationship conclusion already based on these diagrams, but as we can see from correlation matrix (coming up next) things are not so obvious.

```{r echo=FALSE, message=FALSE}
Boston %>%
  keep(is.numeric) %>%                     # Keep only numeric columns
  gather() %>%                             # Convert to key-value pairs
  ggplot(aes(value)) +                     # Plot the values
    facet_wrap(~ key, scales = "free") +   # In separate panels
    geom_density()                         # as density
```

Third, with a type of correlation matrix we can observe the relationships between the variables. I used a `mixed` version of the `corrplot` which was introduced in the DataCamp exercises. From this one you can visually locate the varibles with positive (&rarr; 1) and negative (&rarr; -1) relations from the top triangle and check their coefficients from the lower triangle. We can observe that variable pair (`rad`, `tax`) have the highest (> 0.9) positive relationship. Also pairs (`indus`,`nox`), (`indus`,`tax`), (`nox`,`age`) and (`rm`,`mediv`) have high posivite relationship (> 0.7). On the high negative relationships (< -0.7) we can mention pairs (`indus`,`dis`), (`nox`,`dis`), (`age`,`dis`) and (`lstat`,`medv`). Also noteworthy is that variable `chas` does not seem to correlate positively or negatively with any other variable in the dataset.

```{r echo=FALSE, message=FALSE}
cor_matrix<-cor(Boston) %>% round(digits=3)
corrplot.mixed(cor_matrix, tl.cex=0.7)
```

### Step 4 - Standardizing the Boston dataset

After scaling the variable values are scaled to normal distribution with 0 mean (or very close) and variance 1. Meaning the amout of values are equally numbered on both sides of 0 mean. This can be observed from the density plots after the summary (compare to similar density plot above).
```{r echo=FALSE, message=TRUE}
boston_scaled <- scale(Boston)
summary(boston_scaled)
boston_scaled <- as.data.frame(boston_scaled)
boston_scaled %>%
  keep(is.numeric) %>%                     # Keep only numeric columns
  gather() %>%                             # Convert to key-value pairs
  ggplot(aes(value)) +                     # Plot the values
    facet_wrap(~ key, scales = "free") +   # In separate panels
    geom_density()                         # as density
```

Then the based on the quantiles of the scaled crime rate variable `crim` a categorical variable is created. Original variable `crim` is removed and the new created variable `crime` is added to the date set. Here is the stucture of the modified dataset.
```{r echo=FALSE, message=TRUE}
bins <- quantile(boston_scaled$crim)
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label=c("low","med_low","med_high","high"))
boston_scaled <- dplyr::select(boston_scaled, -crim)
boston_scaled <- data.frame(boston_scaled, crime)
str(boston_scaled)
```

Dataset is split into two, so that 80% of the data is in train set and 20% in the test set.

```{r echo=FALSE, message=TRUE}
n <- nrow(boston_scaled)
ind <- sample(n,  size = n * 0.8)
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]
dim(train)
dim(test)
```

### Step 5 - Fitting the LDA

Here the LDA model is fitted with the train set. The model summary is printed. We can see that LD1 covers more than 0.95 of the group variance. Next the LDA is plotted as biplot. I used just 1 discriminant. There is something going on with the arrows, they don't work on the same scale as in the DataCamp. In order to even get some of the arrows visible I needed scale way more. (the number of discriminants did not have any effect on this property)

```{r echo=TRUE, message=FALSE, warning=FALSE}
lda.fit <- lda(crime ~ ., data = train)
lda.fit

lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
classes <- as.numeric(train$crime)
plot(classes, dimen = 1, col=classes, pch=classes)
lda.arrows(lda.fit, myscale = 16)
```

### Step 6 - Predicting with the LDA

The LDA model fitted in previous step is used to predict the crime class for test data after real test values are removed from the data. Then the predicted class results are compared against the known true values in the table below. We can see that on the test data the predicted values for class `high` are almost perfect. Class `med_high` seems to have the worst prediction precision since almost half of the values are predicted to other classes. Class `med_low` gets almost similarly bad result where 2/3 of the values are predicted to wrong classes. Class `low` has the second best prediction precision but for that almost 1/3 get predicted to wrong class.

```{r echo=FALSE, message=FALSE, warning=FALSE}
correct_classes <- test[,'crime']
test <- dplyr::select(test, -crime)
lda.pred <- predict(lda.fit, newdata = test)
table(correct = correct_classes, predicted = lda.pred$class)
```

### Step 7 - Clustering with K-means

First Boston dataset is reloaded and scaled again.

```{r echo=FALSE, message=FALSE, warning=FALSE}
bs2 <- as.data.frame(scale(Boston))
str(bs2)
```

Then the euclidian distances between the scaled observations is computed.
```{r echo=FALSE, message=FALSE, warning=FALSE}
bs2_dist_eu <- dist(bs2)
summary(bs2_dist_eu)
```

Simple initial run of K-means algorithm with randomly chosen number of cluster centers (5). The pairs plot is hard to read when all variables are compared with each other. Splitting the data for pairs plot makes it more readable but you will then miss the comparson between all variables.

```{r echo=FALSE, message=FALSE, warning=FALSE}
km <- kmeans(bs2, centers = 5)
pairs(bs2, col = km$cluster)
```

To determine the optimal number of clusters total WCSS (within cluster sum of squares) can be used for help. Here I use 14 as the maximum cluster, just based on the number of variables in the dataset, and quick plot the tWCSS results. The optimal number of clusters is when the tWCSS drops dramatically, and here we get the same result (2 clusters) as in the Data Camp exercise.

```{r echo=FALSE, message=FALSE, warning=FALSE}
set.seed(666)
k_max = 14
twcss <- sapply(1:k_max, function(k){kmeans(bs2, k)$tot.withinss})
qplot(x = 1:k_max, y = twcss, geom = 'line')
```
Running the K-means algorithm again but now with 2 cluster as per optimal suggestion. Plotting the results with pairs to see all variables compared and split to two clusters.

```{r echo=FALSE, message=FALSE, warning=FALSE}
km <- kmeans(bs2, centers = 2)
pairs(bs2, col = km$cluster)

```

### Step Bonus - Finding classes to train LDA with K-means

I found the instructions for this bonus exercise a little ambiguous, so I took some freedom since it is not clearly stated how some of the things should be done.  

So, first I took the Boston dataset and scaled it. Did nothing to `crim`, no categorization.

```{r echo=FALSE, message=FALSE, warning=FALSE}
bs3 <- scale(Boston)
bs3 <- as.data.frame(bs3)
summary(bs3)
```

Then I ran k-means on the scaled data with 3 clusters specified. Showing the pairs plot. Took the cluster vector from results and added that as new column to origal scaled data.

```{r echo=FALSE, message=FALSE, warning=FALSE}
km <- kmeans(bs3, centers = 3)
pairs(bs3, col = km$cluster)
cluster <- km$cluster
bs3 <- data.frame(bs3, cluster)
str(bs3)
```

I did the same split into train and test dataset, as we did here earlier, so that I can see how well is the model predicting. Most influencial linear separators being `medv` and `chas`.

```{r echo=FALSE, message=FALSE, warning=FALSE}
n <- nrow(bs3)
ind <- sample(n,  size = n * 0.8)
train <- bs3[ind,]
test <- bs3[-ind,]

lda.fit <- lda(cluster ~ ., data = train)
lda.fit
classes <- as.numeric(train$cluster)
plot(classes, dimen = 2, col=classes, pch=classes)
lda.arrows(lda.fit, myscale = 3)

correct_classes <- test[,'cluster']
test <- dplyr::select(test, -cluster)
lda.pred <- predict(lda.fit, newdata = test)
table(correct = correct_classes, predicted = lda.pred$class)
```

And we can see that the precidtion is quite precise. But what these 3 classes should represent with this data (when above we were predicting crime).


## Data Wrangling

In this part it was needed to read two datasets from the internet. Then rename the column names in both datasets, create 2 new variables and the combine the data in to one dataset and store it. I had some problems when reading the stored data, I did not get the same amount of observations that was in the file (I got 164 instead of 195 in the file). Did not see really anything wrong in the file, but I turned quotes to it default value (TRUE) when saving the data, and after that the data with correct amount of observations could be read.

The code for the date can be found from [here](https://github.com/ittobor/IODS-project/blob/master/data/create_human.R)  
And the data can be found from [here](https://github.com/ittobor/IODS-project/blob/master/data/human.csv)

